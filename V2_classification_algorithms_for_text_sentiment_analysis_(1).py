# -*- coding: utf-8 -*-
"""classification_algorithms_for_text_sentiment_analysis (1).ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1w6ILS2hVngQX9nM0LCQOKmDIqaBobQ71
"""

import pandas as pd
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.model_selection import train_test_split
from sklearn.linear_model import LogisticRegression
from sklearn.naive_bayes import MultinomialNB
from sklearn.svm import SVC
from sklearn.metrics import accuracy_score

# Step 1: Load the datasets
train_data = pd.read_csv('train.csv')
test_data = pd.read_csv('test.csv')

# Step 2: Preprocess the data (assuming basic preprocessing)
def preprocess_text(text):
    # Convert text to lowercase
    text = text.lower()
    # Remove punctuation and numbers
    text = ''.join([char for char in text if char.isalpha() or char.isspace()])
    return text

train_data['processed_text'] = train_data['text'].apply(preprocess_text)
test_data['processed_text'] = test_data['text'].apply(preprocess_text)

# Step 3: Feature extraction using various methods (BoW, TF-IDF, embeddings, etc.)
# Option 1: TF-IDF vectors
vectorizer = TfidfVectorizer()
X_train_tfidf = vectorizer.fit_transform(train_data['processed_text'])
X_test_tfidf = vectorizer.transform(test_data['processed_text'])

# Uncomment for Option 2: Word embeddings
# X_train = # Extract word embeddings for train_data
# X_test = # Extract word embeddings for test_data

y_train = train_data['label']

# Step 4: Split the data into training and validation sets
X_train_tfidf, X_val_tfidf, y_train, y_val = train_test_split(
    X_train_tfidf, y_train, test_size=0.2, random_state=42
)

# Step 5: Train classifiers (Logistic Regression, Naive Bayes, SVM, etc.)
# Option 1: Logistic Regression
classifier_lr_tfidf = LogisticRegression(max_iter=50)
classifier_lr_tfidf.fit(X_train_tfidf, y_train)
y_pred_lr_tfidf = classifier_lr_tfidf.predict(X_val_tfidf)
accuracy_lr_tfidf = accuracy_score(y_val, y_pred_lr_tfidf)
print('Logistic Regression (TF-IDF) Accuracy:', accuracy_lr_tfidf)

# Option 2: Naive Bayes Classifier
classifier_nb_tfidf = MultinomialNB()
classifier_nb_tfidf.fit(X_train_tfidf, y_train)
y_pred_nb_tfidf = classifier_nb_tfidf.predict(X_val_tfidf)
accuracy_nb_tfidf = accuracy_score(y_val, y_pred_nb_tfidf)
print('Naive Bayes (TF-IDF) Accuracy:', accuracy_nb_tfidf)

# Option 3: SVM Classifier
classifier_svm_tfidf = SVC(max_iter=50)
classifier_svm_tfidf.fit(X_train_tfidf, y_train)
y_pred_svm_tfidf = classifier_svm_tfidf.predict(X_val_tfidf)
accuracy_svm_tfidf = accuracy_score(y_val, y_pred_svm_tfidf)
print('SVM (TF-IDF) Accuracy:', accuracy_svm_tfidf)

# Step 6: Make predictions on the test set
test_predictions_lr_tfidf = classifier_lr_tfidf.predict(X_test_tfidf)
test_predictions_nb_tfidf = classifier_nb_tfidf.predict(X_test_tfidf)
test_predictions_svm_tfidf = classifier_svm_tfidf.predict(X_test_tfidf)

# Additional Comments:
# We chose TF-IDF as a feature because it captures the importance of words in each document
# relative to the entire corpus. TF-IDF considers both the term frequency (TF) and inverse
# document frequency (IDF), allowing us to identify the most distinctive words for each document.
# In the context of sentiment analysis, TF-IDF can help identify words or phrases that are
# indicative of positive or negative sentiment. This feature can be valuable in capturing the
# sentiment expressed in the reviews.
#
# Combining TF-IDF vectors and topic modeling can be achieved through different approaches. One
# possible approach is using a voting system, where each model's prediction (e.g., TF-IDF-based
# classifier and topic modeling-based classifier) is considered as a vote. The final prediction
# can be determined based on majority voting, equal votes, or a weighted voting system, where more
# trustworthy models are given more weight.
#
# Another approach is stacking models, which involves using the predictions from the initial models
# (e.g., TF-IDF-based classifier and topic modeling-based classifier) as input features to a
# second-level model, also known as a "meta-learner." The meta-learner can be trained to make the
# final prediction based on the outputs of the initial models. This approach allows for the
# combination of different features and can potentially improve the overall predictive performance
# by learning from the strengths of each model.